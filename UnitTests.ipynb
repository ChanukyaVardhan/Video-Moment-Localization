{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f34ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "198b54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAEncoder(nn.Module):\n",
    "    def __init__(self, Nq, D):\n",
    "        super(CGAEncoder, self).__init__()\n",
    "        self.D, self.attn_weights = D, None\n",
    "        self.W_q = nn.Linear(D, D)\n",
    "        self.W_k = nn.Linear(D, D)\n",
    "        self.W_v = nn.Linear(D, D)\n",
    "    \n",
    "    # query: (B, Nq, D), query_mask: (B, Nq), key: (B, L, L, D), value: (B, L, L, D), kv_mask: (B, L, L)\n",
    "    def forward(self, query, query_mask, key, value, kv_mask):\n",
    "        B, L, D =  key.shape[0], key.shape[1], key.shape[3]\n",
    "        # key, value: (B, L, L, D) -> key, value: (B, L*L, D)\n",
    "        key, value = torch.reshape(key, (B, L*L, D)), torch.reshape(value, (B, L*L, D))\n",
    "        query, key, value  = self.W_q(query), self.W_k(key), self.W_v(value) \n",
    "        # query: (B, Nq, D), key, value: (B, L*L, D) -> attn_weights: (B, Nq, L*L)\n",
    "        if query.dim() == 2:\n",
    "            query = query.unsqueeze(1)\n",
    "        attn_weights = torch.matmul(query, torch.transpose(key, 2, 1))/math.sqrt(D)\n",
    "        # kv_mask: (B, L, L) -> kv_mask: (B, 1, L*L)\n",
    "        mask = kv_mask.reshape(B, L*L).unsqueeze(1)\n",
    "        if query_mask is not None:\n",
    "            # query_mask: (B, Nq) -> query_mask: (B, Nq, 1)\n",
    "            query_mask = query_mask.float().unsqueeze(2)\n",
    "            # query_mask: (B, Nq, 1), kv_mask: (B, 1, L*L) -> mask: (B, Nq, L*L)\n",
    "            mask = mask*query_mask\n",
    "        # attn_weights: (B, Nq, L*L)\n",
    "        attn_weights = attn_weights * mask\n",
    "        print(attn_weights.shape, mask.shape)\n",
    "        # attn_weights: (B, Nq, L*L)\n",
    "        attn_weights = attn_weights.masked_fill(mask == 0, -1e9)\n",
    "        # attn_weights: (B, Nq, L*L)\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "        # attn_weights: (B, Nq, L*L), value: (B, L*L, D) -> attn_out: (B, Nq, D)\n",
    "        attn_out = torch.matmul(attn_weights, value)\n",
    "        self.attn_weights = attn_weights\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7db0dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, L, L, D, Nq = 3, 4, 4, 8, 5\n",
    "query, key = torch.rand(B, Nq, D), torch.rand(B,L,L,D)\n",
    "query_mask, kv_mask = torch.ones(B, Nq),torch.ones(B, L, L) \n",
    "query_mask[:, -1], kv_mask[:, :, -1] = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e466fa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 16]) torch.Size([3, 5, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3037,  0.0427, -0.0457,  0.0515,  0.0088,  0.2548, -0.4338,\n",
       "          -0.0245],\n",
       "         [ 0.3043,  0.0422, -0.0453,  0.0521,  0.0080,  0.2542, -0.4340,\n",
       "          -0.0245],\n",
       "         [ 0.3021,  0.0448, -0.0416,  0.0572,  0.0045,  0.2549, -0.4374,\n",
       "          -0.0211],\n",
       "         [ 0.3027,  0.0451, -0.0430,  0.0571,  0.0072,  0.2544, -0.4340,\n",
       "          -0.0224],\n",
       "         [ 0.3407,  0.0574, -0.0623,  0.0828, -0.0292,  0.2581, -0.4014,\n",
       "          -0.0187]],\n",
       "\n",
       "        [[ 0.4021,  0.1335, -0.1149,  0.2367, -0.0140,  0.2556, -0.3511,\n",
       "           0.0902],\n",
       "         [ 0.3987,  0.1343, -0.1117,  0.2381, -0.0118,  0.2536, -0.3526,\n",
       "           0.0911],\n",
       "         [ 0.3958,  0.1379, -0.1113,  0.2357, -0.0118,  0.2583, -0.3545,\n",
       "           0.0912],\n",
       "         [ 0.3964,  0.1372, -0.1109,  0.2380, -0.0126,  0.2560, -0.3539,\n",
       "           0.0928],\n",
       "         [ 0.3657,  0.1286, -0.0823,  0.1888, -0.0025,  0.2950, -0.3846,\n",
       "           0.0573]],\n",
       "\n",
       "        [[ 0.2624,  0.0963, -0.0411,  0.1983,  0.0965,  0.1712, -0.4266,\n",
       "           0.1199],\n",
       "         [ 0.2648,  0.0968, -0.0426,  0.2006,  0.0956,  0.1721, -0.4258,\n",
       "           0.1219],\n",
       "         [ 0.2663,  0.0985, -0.0434,  0.2001,  0.0942,  0.1762, -0.4251,\n",
       "           0.1211],\n",
       "         [ 0.2638,  0.0987, -0.0431,  0.1989,  0.0966,  0.1754, -0.4268,\n",
       "           0.1209],\n",
       "         [ 0.2985,  0.0947, -0.0546,  0.1878,  0.0800,  0.2056, -0.4382,\n",
       "           0.0973]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = CGAEncoder(Nq, D)\n",
    "encoder(query, query_mask, key, key, kv_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "97ad6ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0891, 0.0750, 0.0907, 0.0000, 0.0806, 0.0917, 0.0785, 0.0000,\n",
      "          0.0903, 0.0872, 0.0722, 0.0000, 0.0788, 0.0798, 0.0861, 0.0000],\n",
      "         [0.0905, 0.0760, 0.0879, 0.0000, 0.0796, 0.0909, 0.0780, 0.0000,\n",
      "          0.0912, 0.0866, 0.0713, 0.0000, 0.0807, 0.0816, 0.0856, 0.0000],\n",
      "         [0.0858, 0.0804, 0.0826, 0.0000, 0.0769, 0.0923, 0.0864, 0.0000,\n",
      "          0.0842, 0.0848, 0.0704, 0.0000, 0.0809, 0.0836, 0.0915, 0.0000],\n",
      "         [0.0856, 0.0779, 0.0863, 0.0000, 0.0810, 0.0911, 0.0819, 0.0000,\n",
      "          0.0849, 0.0839, 0.0729, 0.0000, 0.0832, 0.0824, 0.0888, 0.0000],\n",
      "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
      "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]],\n",
      "\n",
      "        [[0.0783, 0.0919, 0.0909, 0.0000, 0.0912, 0.0825, 0.0747, 0.0000,\n",
      "          0.0817, 0.0798, 0.0808, 0.0000, 0.0756, 0.0922, 0.0804, 0.0000],\n",
      "         [0.0798, 0.0889, 0.0909, 0.0000, 0.0874, 0.0817, 0.0786, 0.0000,\n",
      "          0.0837, 0.0822, 0.0783, 0.0000, 0.0775, 0.0917, 0.0792, 0.0000],\n",
      "         [0.0766, 0.0884, 0.0877, 0.0000, 0.0911, 0.0821, 0.0815, 0.0000,\n",
      "          0.0844, 0.0854, 0.0778, 0.0000, 0.0778, 0.0865, 0.0807, 0.0000],\n",
      "         [0.0785, 0.0869, 0.0900, 0.0000, 0.0906, 0.0823, 0.0821, 0.0000,\n",
      "          0.0821, 0.0847, 0.0779, 0.0000, 0.0776, 0.0883, 0.0790, 0.0000],\n",
      "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
      "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]],\n",
      "\n",
      "        [[0.0752, 0.0876, 0.0891, 0.0000, 0.0832, 0.0769, 0.0831, 0.0000,\n",
      "          0.0735, 0.0877, 0.0905, 0.0000, 0.0790, 0.0905, 0.0837, 0.0000],\n",
      "         [0.0740, 0.0870, 0.0868, 0.0000, 0.0835, 0.0753, 0.0867, 0.0000,\n",
      "          0.0736, 0.0881, 0.0916, 0.0000, 0.0775, 0.0904, 0.0855, 0.0000],\n",
      "         [0.0750, 0.0882, 0.0857, 0.0000, 0.0841, 0.0778, 0.0873, 0.0000,\n",
      "          0.0760, 0.0860, 0.0879, 0.0000, 0.0771, 0.0896, 0.0853, 0.0000],\n",
      "         [0.0739, 0.0900, 0.0894, 0.0000, 0.0857, 0.0770, 0.0857, 0.0000,\n",
      "          0.0766, 0.0841, 0.0876, 0.0000, 0.0768, 0.0898, 0.0834, 0.0000],\n",
      "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
      "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(encoder.attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ea9d2a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 16]) torch.Size([3, 1, 16])\n"
     ]
    }
   ],
   "source": [
    "query_sent = torch.rand(B, D)\n",
    "out = encoder(query_sent, None, key, key, kv_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f5d01ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8])\n"
     ]
    }
   ],
   "source": [
    "print(out.squeeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d6fbfd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 16])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(B, 1, D)\n",
    "b = torch.rand(B, D, L*L)\n",
    "c = torch.bmm(a, b)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69418f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706de2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
